"""Agent runtime: executes agent prompts against configured LLMs.

Phase 9.1: Per-agent explicit model selection.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any, Protocol

from langchain_core.messages import SystemMessage, HumanMessage

class AgentLike(Protocol):
    system_prompt: str

from .backends import create_chat_model, identity_from_config, ModelIdentity
from .config import AgentModelConfig
from .prompts import SYSTEM_NO_COT


@dataclass
class AgentRunResult:
    content: str
    model: ModelIdentity
    raw: dict[str, Any] | None = None


class AgentRunner:
    def __init__(self) -> None:
        self._model_cache: dict[tuple[str, str, float, int], Any] = {}

    def _get_model(self, cfg: AgentModelConfig):
        key = (cfg.provider.value, cfg.model, float(cfg.temperature), int(cfg.max_tokens))
        if key not in self._model_cache:
            try:
                self._model_cache[key] = create_chat_model(cfg)
            except Exception as exc:
                self._model_cache[key] = exc
        return self._model_cache[key]

    def run_text(self, *, agent: AgentLike, model_cfg: AgentModelConfig, user_prompt: str) -> AgentRunResult:
        model_id = identity_from_config(model_cfg)

        llm = self._get_model(model_cfg)
        if isinstance(llm, Exception):
            err = (
                "LLM initialization failed. This output is UNVERIFIED and was not generated by the model.\n\n"
                f"Provider: {model_id.provider}\nModel: {model_id.model}\n"
                f"Error: {type(llm).__name__}: {llm}"
            )
            return AgentRunResult(content=err, model=model_id, raw={"error": str(llm)})

        try:
            response = llm.invoke(
                [
                    SystemMessage(content=SYSTEM_NO_COT + "\n\n" + agent.system_prompt.strip()),
                    HumanMessage(content=user_prompt),
                ]
            )
            text = getattr(response, "content", str(response))
            return AgentRunResult(content=text, model=model_id)
        except Exception as exc:
            # Do not claim success. Record the failure as part of the audit trail.
            err = (
                "LLM invocation failed. This output is UNVERIFIED and was not generated by the model.\n\n"
                f"Provider: {model_id.provider}\nModel: {model_id.model}\n"
                f"Error: {type(exc).__name__}: {exc}"
            )
            return AgentRunResult(content=err, model=model_id, raw={"error": str(exc)})

    def run_json(self, *, agent: AgentLike, model_cfg: AgentModelConfig, user_prompt: str) -> AgentRunResult:
        """Best-effort JSON mode.

        We ask for strict JSON and parse it. If parsing fails, `raw` is None.
        """

        json_prompt = (
            user_prompt
            + "\n\nOUTPUT FORMAT:\n"
            + "Return STRICT JSON only (no markdown)."
        )

        result = self.run_text(agent=agent, model_cfg=model_cfg, user_prompt=json_prompt)

        parsed: dict[str, Any] | None = None
        try:
            parsed = json.loads(result.content)
        except Exception:
            parsed = None

        result.raw = parsed
        return result
